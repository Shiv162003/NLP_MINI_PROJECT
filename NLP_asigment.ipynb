{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b5b267",
   "metadata": {},
   "source": [
    "# name-shivansh naui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90534d60",
   "metadata": {},
   "source": [
    "# Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb61ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Initialize NLTK resources\n",
    "#it should always be uptodated \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e53d28",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a46fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalnum()]\n",
    "    return ' '.join(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f15593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each review\n",
    "data['cleaned_review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "# Replace original review column with cleaned_review\n",
    "data['review'] = data['cleaned_review']\n",
    "\n",
    "# Drop the cleaned_review column if you don't need it anymore\n",
    "data.drop(columns=['cleaned_review'], inplace=True)\n",
    "\n",
    "# Save the cleaned data back to the same file\n",
    "data.to_csv('Cleaned_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d1cc5",
   "metadata": {},
   "source": [
    "#  Vectorize Data Using:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3670a",
   "metadata": {},
   "source": [
    "# a) CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f627a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv('Cleaned_Data.csv')\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "count_vectors = count_vectorizer.fit_transform(data['review'])\n",
    "\n",
    "# Store the sparse matrix directly in the DataFrame\n",
    "data['count_vectors'] = list(count_vectors)\n",
    "\n",
    "# Save the data with count vectors to a new CSV file\n",
    "data.to_csv('data_with_count_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c5b9a",
   "metadata": {},
   "source": [
    "#    b) TFIDFVectorizer\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0f83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Cleaned_Data.csv')\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(data['review'])\n",
    "\n",
    "# Store the sparse matrix directly in the DataFrame\n",
    "data['tfidf_vectors'] = list(tfidf_vectors)\n",
    "\n",
    "# Save the data with TF-IDF vectors to a new CSV file\n",
    "data.to_csv('data_with_tfidf_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84662123",
   "metadata": {},
   "source": [
    "#  c) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa7a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load your review dataset\n",
    "data = pd.read_csv(\"Cleaned_Data.csv\")  # Replace \"your_dataset.csv\" with the actual file name\n",
    "\n",
    "\n",
    "reviews = data['review']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81eb2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews into lists of words\n",
    "tokenized_reviews = [review.split() for review in reviews]\n",
    "\n",
    "# Train a Word2Vec model on your tokenized data\n",
    "model = Word2Vec(sentences=tokenized_reviews, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert a review text to a vector using the trained Word2Vec model\n",
    "def vectorize_review(review):\n",
    "    words = review.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Average of word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zeros if no valid words are found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8930a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize each review in the dataset\n",
    "review_vectors = [vectorize_review(review) for review in reviews]\n",
    "\n",
    "# Create a DataFrame to hold the review vectors\n",
    "vector_df = pd.DataFrame(review_vectors)\n",
    "\n",
    "# Concatenate the vector DataFrame with the original dataset\n",
    "vectorized_data = pd.concat([data, vector_df], axis=1)\n",
    "\n",
    "# Save the vectorized data to a new CSV file\n",
    "vectorized_data.to_csv(\"vectorized_reviews_custom_word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fc412",
   "metadata": {},
   "source": [
    "#     d) GoogleNews Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6fdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the Google News Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Save the model to a file\n",
    "model.save(\"google_news_word2vec.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c863c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the saved KeyedVectors model\n",
    "model = KeyedVectors.load(\"google_news_word2vec.model\")\n",
    "\n",
    "# Load your review dataset\n",
    "data = pd.read_csv(\"Cleaned_Data.csv\") \n",
    "reviews = data['review']\n",
    "\n",
    "# Function to convert a review text to a vector using the KeyedVectors model\n",
    "def vectorize_review(review):\n",
    "    words = review.split()\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            vector.append(model[word])\n",
    "    if vector:\n",
    "        return sum(vector) / len(vector)  # Average of word vectors\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Vectorize each review in the dataset\n",
    "review_vectors = [vectorize_review(review) for review in reviews]\n",
    "\n",
    "# Create a DataFrame to hold the review vectors\n",
    "vector_df = pd.DataFrame(review_vectors)\n",
    "\n",
    "# Concatenate the vector DataFrame with the original dataset\n",
    "vectorized_data = pd.concat([data, vector_df], axis=1)\n",
    "\n",
    "# Save the vectorized data to a new CSV file\n",
    "vectorized_data.to_csv(\"vectorized_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646b491",
   "metadata": {},
   "source": [
    "# Use following ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1fb4e",
   "metadata": {},
   "source": [
    "# a) Logistic Regression\n",
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cdc4425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88      4961\n",
      "    positive       0.88      0.89      0.88      5039\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"data_with_count_vectors.csv\")\n",
    "\n",
    "# Extract the review text, sentiment, and count_vectors\n",
    "reviews = data['review']\n",
    "sentiments = data['sentiment']\n",
    "\n",
    "# Use CountVectorizer to convert text data to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectors = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_vectors, sentiments, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8493770",
   "metadata": {},
   "source": [
    "#  TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2a913d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.66      4961\n",
      "    positive       0.00      0.00      0.00      5039\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.25      0.50      0.33     10000\n",
      "weighted avg       0.25      0.50      0.33     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data_with_tfidf_vectors.csv')\n",
    "\n",
    "# Function to convert the string representation to a sparse matrix\n",
    "def parse_sparse_string(s):\n",
    "    data = re.findall(r\"\\((\\d+), ([\\d.]+)\\)\", s)\n",
    "    indices = [int(idx) for idx, _ in data]\n",
    "    values = [float(val) for _, val in data]\n",
    "    return csr_matrix((values, ([0] * len(indices), indices)), shape=(1, max(indices) + 1))\n",
    "\n",
    "# Convert the string TF-IDF vectors to sparse matrices\n",
    "X_sparse = data['tfidf_vectors'].apply(parse_sparse_string)\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X = [x.toarray().flatten() for x in X_sparse]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1048be3",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2832bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.87      4961\n",
      "    positive       0.86      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv('vectorized_reviews_custom_word2vec.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(['review', 'sentiment'], axis=1)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiments on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fb949",
   "metadata": {},
   "source": [
    "# GoogleNews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a9689ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85      4961\n",
      "    positive       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the data (replace 'data.csv' with your actual dataset file)\n",
    "data = pd.read_csv('data_with_google_news_word2vec..csv')\n",
    "\n",
    "# Separate features (vectors) and labels\n",
    "X = data.drop(['review', 'sentiment'], axis=1)  # Features are columns after 'review' and 'sentiment'\n",
    "y = data['sentiment']  # Sentiment labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967ab8c",
   "metadata": {},
   "source": [
    "\n",
    "#  SVC with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263461c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.81      0.85       996\n",
      "    positive       0.83      0.89      0.86      1004\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"data_with_count_vectors.csv\")\n",
    "data=data.head(10000)\n",
    "# Extract the review text, sentiment, and count_vectors\n",
    "reviews = data['review']\n",
    "sentiments = data['sentiment']\n",
    "\n",
    "# Use CountVectorizer to convert text data to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectors = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_vectors, sentiments, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVC model\n",
    "model = SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5f265",
   "metadata": {},
   "source": [
    "# SVC with TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee289c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.86      0.64      4961\n",
      "    positive       0.55      0.18      0.27      5039\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.53      0.52      0.45     10000\n",
      "weighted avg       0.53      0.51      0.45     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data_with_tfidf_vectors.csv')\n",
    "\n",
    "# Function to convert the string representation to a sparse matrix\n",
    "def parse_sparse_string(s):\n",
    "    data = re.findall(r\"\\((\\d+), ([\\d.]+)\\)\", s)\n",
    "    indices = [int(idx) for idx, _ in data]\n",
    "    values = [float(val) for _, val in data]\n",
    "    return csr_matrix((values, ([0] * len(indices), indices)), shape=(1, max(indices) + 1))\n",
    "\n",
    "# Convert the string TF-IDF vectors to sparse matrices\n",
    "X_sparse = data['tfidf_vectors'].apply(parse_sparse_string)\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X = [x.toarray().flatten() for x in X_sparse]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Classification model\n",
    "model = SVC()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de789fe2",
   "metadata": {},
   "source": [
    "# SVC with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befab4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.86      0.87      4961\n",
      "    positive       0.87      0.89      0.88      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv('vectorized_reviews_custom_word2vec.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(['review', 'sentiment'], axis=1)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Support Vector Classifier (SVC) model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiments on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304845be",
   "metadata": {},
   "source": [
    "# SVC Google new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61914c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.86      4961\n",
      "    positive       0.86      0.87      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the data (replace 'data.csv' with your actual dataset file)\n",
    "data = pd.read_csv('data_with_google_news_word2vec..csv')\n",
    "\n",
    "# Separate features (vectors) and labels\n",
    "X = data.drop(['review', 'sentiment'], axis=1)  # Features are columns after 'review' and 'sentiment'\n",
    "y = data['sentiment']  # Sentiment labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Classifier (SVC) model\n",
    "model = SVC()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f9429",
   "metadata": {},
   "source": [
    "# RF with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8e896db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85       996\n",
      "    positive       0.85      0.86      0.85      1004\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"data_with_count_vectors.csv\")\n",
    "data = data.head(10000)\n",
    "\n",
    "# Extract the review text, sentiment, and count_vectors\n",
    "reviews = data['review']\n",
    "sentiments = data['sentiment']\n",
    "\n",
    "# Use CountVectorizer to convert text data to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectors = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_vectors, sentiments, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e23f8",
   "metadata": {},
   "source": [
    "# RF TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7241b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.53      0.52      4961\n",
      "    positive       0.52      0.51      0.52      5039\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.52      0.52      0.52     10000\n",
      "weighted avg       0.52      0.52      0.52     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data_with_tfidf_vectors.csv')\n",
    "\n",
    "# Function to convert the string representation to a sparse matrix\n",
    "def parse_sparse_string(s):\n",
    "    data = re.findall(r\"\\((\\d+), ([\\d.]+)\\)\", s)\n",
    "    indices = [int(idx) for idx, _ in data]\n",
    "    values = [float(val) for _, val in data]\n",
    "    return csr_matrix((values, ([0] * len(indices), indices)), shape=(1, max(indices) + 1))\n",
    "\n",
    "# Convert the string TF-IDF vectors to sparse matrices\n",
    "X_sparse = data['tfidf_vectors'].apply(parse_sparse_string)\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X = [x.toarray().flatten() for x in X_sparse]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803827d",
   "metadata": {},
   "source": [
    " #   Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e836162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.82      0.84      4961\n",
      "    positive       0.83      0.86      0.84      5039\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv('vectorized_reviews_custom_word2vec.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(['review', 'sentiment'], axis=1)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiments on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0879127",
   "metadata": {},
   "source": [
    "#   GoogleNews Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c02d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8102\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.80      0.81      4961\n",
      "    positive       0.81      0.82      0.81      5039\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the data (replace 'data.csv' with your actual dataset file)\n",
    "data = pd.read_csv('data_with_google_news_word2vec..csv')\n",
    "\n",
    "# Separate features (vectors) and labels\n",
    "X = data.drop(['review', 'sentiment'], axis=1)  # Features are columns after 'review' and 'sentiment'\n",
    "y = data['sentiment']  # Sentiment labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
